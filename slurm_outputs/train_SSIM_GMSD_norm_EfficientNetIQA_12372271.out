============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
Running on gcn39.local.snellius.surf.nl
Sat Jun 14 12:42:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:31:00.0 Off |                    0 |
| N/A   29C    P0             50W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
               total        used        free      shared  buff/cache   available
Mem:           503Gi        53Gi       403Gi       325Mi        53Gi       449Gi
Swap:             0B          0B          0B
Checking whether the GPU is available
GPU available? True
Starting training for ['SSIM', 'GMSD_norm'] on IQDCNN
/home/mvanderbrugge/.conda/envs/photoacoustic-env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/mvanderbrugge/.conda/envs/photoacoustic-env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)

Training EfficientNetIQA model for metrics: ['SSIM', 'GMSD_norm']
Loaded 573629 train, 25910 val, 339535 test samples from ['SCD_vc_ms', 'SCD_ms_ss32', 'SCD_ms_ss64', 'SCD_ms_ss128', 'SWFD_sc', 'mice', 'pa_experiment_data']
Target metric: ['SSIM', 'GMSD_norm']
Target ranges:
  SSIM: min=0.2486, max=0.9689
  GMSD_norm: min=0.0178, max=0.7650
Using EfficientNetIQAMulti
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 64, 64]             288
       BatchNorm2d-2           [-1, 32, 64, 64]              64
              SiLU-3           [-1, 32, 64, 64]               0
            Conv2d-4           [-1, 32, 64, 64]             288
       BatchNorm2d-5           [-1, 32, 64, 64]              64
              SiLU-6           [-1, 32, 64, 64]               0
 AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0
            Conv2d-8              [-1, 8, 1, 1]             264
              SiLU-9              [-1, 8, 1, 1]               0
           Conv2d-10             [-1, 32, 1, 1]             288
          Sigmoid-11             [-1, 32, 1, 1]               0
SqueezeExcitation-12           [-1, 32, 64, 64]               0
           Conv2d-13           [-1, 16, 64, 64]             512
      BatchNorm2d-14           [-1, 16, 64, 64]              32
           MBConv-15           [-1, 16, 64, 64]               0
           Conv2d-16           [-1, 96, 64, 64]           1,536
      BatchNorm2d-17           [-1, 96, 64, 64]             192
             SiLU-18           [-1, 96, 64, 64]               0
           Conv2d-19           [-1, 96, 32, 32]             864
      BatchNorm2d-20           [-1, 96, 32, 32]             192
             SiLU-21           [-1, 96, 32, 32]               0
AdaptiveAvgPool2d-22             [-1, 96, 1, 1]               0
           Conv2d-23              [-1, 4, 1, 1]             388
             SiLU-24              [-1, 4, 1, 1]               0
           Conv2d-25             [-1, 96, 1, 1]             480
          Sigmoid-26             [-1, 96, 1, 1]               0
SqueezeExcitation-27           [-1, 96, 32, 32]               0
           Conv2d-28           [-1, 24, 32, 32]           2,304
      BatchNorm2d-29           [-1, 24, 32, 32]              48
           MBConv-30           [-1, 24, 32, 32]               0
           Conv2d-31          [-1, 144, 32, 32]           3,456
      BatchNorm2d-32          [-1, 144, 32, 32]             288
             SiLU-33          [-1, 144, 32, 32]               0
           Conv2d-34          [-1, 144, 32, 32]           1,296
      BatchNorm2d-35          [-1, 144, 32, 32]             288
             SiLU-36          [-1, 144, 32, 32]               0
AdaptiveAvgPool2d-37            [-1, 144, 1, 1]               0
           Conv2d-38              [-1, 6, 1, 1]             870
             SiLU-39              [-1, 6, 1, 1]               0
           Conv2d-40            [-1, 144, 1, 1]           1,008
          Sigmoid-41            [-1, 144, 1, 1]               0
SqueezeExcitation-42          [-1, 144, 32, 32]               0
           Conv2d-43           [-1, 24, 32, 32]           3,456
      BatchNorm2d-44           [-1, 24, 32, 32]              48
  StochasticDepth-45           [-1, 24, 32, 32]               0
           MBConv-46           [-1, 24, 32, 32]               0
           Conv2d-47          [-1, 144, 32, 32]           3,456
      BatchNorm2d-48          [-1, 144, 32, 32]             288
             SiLU-49          [-1, 144, 32, 32]               0
           Conv2d-50          [-1, 144, 16, 16]           3,600
      BatchNorm2d-51          [-1, 144, 16, 16]             288
             SiLU-52          [-1, 144, 16, 16]               0
AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0
           Conv2d-54              [-1, 6, 1, 1]             870
             SiLU-55              [-1, 6, 1, 1]               0
           Conv2d-56            [-1, 144, 1, 1]           1,008
          Sigmoid-57            [-1, 144, 1, 1]               0
SqueezeExcitation-58          [-1, 144, 16, 16]               0
           Conv2d-59           [-1, 40, 16, 16]           5,760
      BatchNorm2d-60           [-1, 40, 16, 16]              80
           MBConv-61           [-1, 40, 16, 16]               0
           Conv2d-62          [-1, 240, 16, 16]           9,600
      BatchNorm2d-63          [-1, 240, 16, 16]             480
             SiLU-64          [-1, 240, 16, 16]               0
           Conv2d-65          [-1, 240, 16, 16]           6,000
      BatchNorm2d-66          [-1, 240, 16, 16]             480
             SiLU-67          [-1, 240, 16, 16]               0
AdaptiveAvgPool2d-68            [-1, 240, 1, 1]               0
           Conv2d-69             [-1, 10, 1, 1]           2,410
             SiLU-70             [-1, 10, 1, 1]               0
           Conv2d-71            [-1, 240, 1, 1]           2,640
          Sigmoid-72            [-1, 240, 1, 1]               0
SqueezeExcitation-73          [-1, 240, 16, 16]               0
           Conv2d-74           [-1, 40, 16, 16]           9,600
      BatchNorm2d-75           [-1, 40, 16, 16]              80
  StochasticDepth-76           [-1, 40, 16, 16]               0
           MBConv-77           [-1, 40, 16, 16]               0
           Conv2d-78          [-1, 240, 16, 16]           9,600
      BatchNorm2d-79          [-1, 240, 16, 16]             480
             SiLU-80          [-1, 240, 16, 16]               0
           Conv2d-81            [-1, 240, 8, 8]           2,160
      BatchNorm2d-82            [-1, 240, 8, 8]             480
             SiLU-83            [-1, 240, 8, 8]               0
AdaptiveAvgPool2d-84            [-1, 240, 1, 1]               0
           Conv2d-85             [-1, 10, 1, 1]           2,410
             SiLU-86             [-1, 10, 1, 1]               0
           Conv2d-87            [-1, 240, 1, 1]           2,640
          Sigmoid-88            [-1, 240, 1, 1]               0
SqueezeExcitation-89            [-1, 240, 8, 8]               0
           Conv2d-90             [-1, 80, 8, 8]          19,200
      BatchNorm2d-91             [-1, 80, 8, 8]             160
           MBConv-92             [-1, 80, 8, 8]               0
           Conv2d-93            [-1, 480, 8, 8]          38,400
      BatchNorm2d-94            [-1, 480, 8, 8]             960
             SiLU-95            [-1, 480, 8, 8]               0
           Conv2d-96            [-1, 480, 8, 8]           4,320
      BatchNorm2d-97            [-1, 480, 8, 8]             960
             SiLU-98            [-1, 480, 8, 8]               0
AdaptiveAvgPool2d-99            [-1, 480, 1, 1]               0
          Conv2d-100             [-1, 20, 1, 1]           9,620
            SiLU-101             [-1, 20, 1, 1]               0
          Conv2d-102            [-1, 480, 1, 1]          10,080
         Sigmoid-103            [-1, 480, 1, 1]               0
SqueezeExcitation-104            [-1, 480, 8, 8]               0
          Conv2d-105             [-1, 80, 8, 8]          38,400
     BatchNorm2d-106             [-1, 80, 8, 8]             160
 StochasticDepth-107             [-1, 80, 8, 8]               0
          MBConv-108             [-1, 80, 8, 8]               0
          Conv2d-109            [-1, 480, 8, 8]          38,400
     BatchNorm2d-110            [-1, 480, 8, 8]             960
            SiLU-111            [-1, 480, 8, 8]               0
          Conv2d-112            [-1, 480, 8, 8]           4,320
     BatchNorm2d-113            [-1, 480, 8, 8]             960
            SiLU-114            [-1, 480, 8, 8]               0
AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0
          Conv2d-116             [-1, 20, 1, 1]           9,620
            SiLU-117             [-1, 20, 1, 1]               0
          Conv2d-118            [-1, 480, 1, 1]          10,080
         Sigmoid-119            [-1, 480, 1, 1]               0
SqueezeExcitation-120            [-1, 480, 8, 8]               0
          Conv2d-121             [-1, 80, 8, 8]          38,400
     BatchNorm2d-122             [-1, 80, 8, 8]             160
 StochasticDepth-123             [-1, 80, 8, 8]               0
          MBConv-124             [-1, 80, 8, 8]               0
          Conv2d-125            [-1, 480, 8, 8]          38,400
     BatchNorm2d-126            [-1, 480, 8, 8]             960
            SiLU-127            [-1, 480, 8, 8]               0
          Conv2d-128            [-1, 480, 8, 8]          12,000
     BatchNorm2d-129            [-1, 480, 8, 8]             960
            SiLU-130            [-1, 480, 8, 8]               0
AdaptiveAvgPool2d-131            [-1, 480, 1, 1]               0
          Conv2d-132             [-1, 20, 1, 1]           9,620
            SiLU-133             [-1, 20, 1, 1]               0
          Conv2d-134            [-1, 480, 1, 1]          10,080
         Sigmoid-135            [-1, 480, 1, 1]               0
SqueezeExcitation-136            [-1, 480, 8, 8]               0
          Conv2d-137            [-1, 112, 8, 8]          53,760
     BatchNorm2d-138            [-1, 112, 8, 8]             224
          MBConv-139            [-1, 112, 8, 8]               0
          Conv2d-140            [-1, 672, 8, 8]          75,264
     BatchNorm2d-141            [-1, 672, 8, 8]           1,344
            SiLU-142            [-1, 672, 8, 8]               0
          Conv2d-143            [-1, 672, 8, 8]          16,800
     BatchNorm2d-144            [-1, 672, 8, 8]           1,344
            SiLU-145            [-1, 672, 8, 8]               0
AdaptiveAvgPool2d-146            [-1, 672, 1, 1]               0
          Conv2d-147             [-1, 28, 1, 1]          18,844
            SiLU-148             [-1, 28, 1, 1]               0
          Conv2d-149            [-1, 672, 1, 1]          19,488
         Sigmoid-150            [-1, 672, 1, 1]               0
SqueezeExcitation-151            [-1, 672, 8, 8]               0
          Conv2d-152            [-1, 112, 8, 8]          75,264
     BatchNorm2d-153            [-1, 112, 8, 8]             224
 StochasticDepth-154            [-1, 112, 8, 8]               0
          MBConv-155            [-1, 112, 8, 8]               0
          Conv2d-156            [-1, 672, 8, 8]          75,264
     BatchNorm2d-157            [-1, 672, 8, 8]           1,344
            SiLU-158            [-1, 672, 8, 8]               0
          Conv2d-159            [-1, 672, 8, 8]          16,800
     BatchNorm2d-160            [-1, 672, 8, 8]           1,344
            SiLU-161            [-1, 672, 8, 8]               0
AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0
          Conv2d-163             [-1, 28, 1, 1]          18,844
            SiLU-164             [-1, 28, 1, 1]               0
          Conv2d-165            [-1, 672, 1, 1]          19,488
         Sigmoid-166            [-1, 672, 1, 1]               0
SqueezeExcitation-167            [-1, 672, 8, 8]               0
          Conv2d-168            [-1, 112, 8, 8]          75,264
     BatchNorm2d-169            [-1, 112, 8, 8]             224
 StochasticDepth-170            [-1, 112, 8, 8]               0
          MBConv-171            [-1, 112, 8, 8]               0
          Conv2d-172            [-1, 672, 8, 8]          75,264
     BatchNorm2d-173            [-1, 672, 8, 8]           1,344
            SiLU-174            [-1, 672, 8, 8]               0
          Conv2d-175            [-1, 672, 4, 4]          16,800
     BatchNorm2d-176            [-1, 672, 4, 4]           1,344
            SiLU-177            [-1, 672, 4, 4]               0
AdaptiveAvgPool2d-178            [-1, 672, 1, 1]               0
          Conv2d-179             [-1, 28, 1, 1]          18,844
            SiLU-180             [-1, 28, 1, 1]               0
          Conv2d-181            [-1, 672, 1, 1]          19,488
         Sigmoid-182            [-1, 672, 1, 1]               0
SqueezeExcitation-183            [-1, 672, 4, 4]               0
          Conv2d-184            [-1, 192, 4, 4]         129,024
     BatchNorm2d-185            [-1, 192, 4, 4]             384
          MBConv-186            [-1, 192, 4, 4]               0
          Conv2d-187           [-1, 1152, 4, 4]         221,184
     BatchNorm2d-188           [-1, 1152, 4, 4]           2,304
            SiLU-189           [-1, 1152, 4, 4]               0
          Conv2d-190           [-1, 1152, 4, 4]          28,800
     BatchNorm2d-191           [-1, 1152, 4, 4]           2,304
            SiLU-192           [-1, 1152, 4, 4]               0
AdaptiveAvgPool2d-193           [-1, 1152, 1, 1]               0
          Conv2d-194             [-1, 48, 1, 1]          55,344
            SiLU-195             [-1, 48, 1, 1]               0
          Conv2d-196           [-1, 1152, 1, 1]          56,448
         Sigmoid-197           [-1, 1152, 1, 1]               0
SqueezeExcitation-198           [-1, 1152, 4, 4]               0
          Conv2d-199            [-1, 192, 4, 4]         221,184
     BatchNorm2d-200            [-1, 192, 4, 4]             384
 StochasticDepth-201            [-1, 192, 4, 4]               0
          MBConv-202            [-1, 192, 4, 4]               0
          Conv2d-203           [-1, 1152, 4, 4]         221,184
     BatchNorm2d-204           [-1, 1152, 4, 4]           2,304
            SiLU-205           [-1, 1152, 4, 4]               0
          Conv2d-206           [-1, 1152, 4, 4]          28,800
     BatchNorm2d-207           [-1, 1152, 4, 4]           2,304
            SiLU-208           [-1, 1152, 4, 4]               0
AdaptiveAvgPool2d-209           [-1, 1152, 1, 1]               0
          Conv2d-210             [-1, 48, 1, 1]          55,344
            SiLU-211             [-1, 48, 1, 1]               0
          Conv2d-212           [-1, 1152, 1, 1]          56,448
         Sigmoid-213           [-1, 1152, 1, 1]               0
SqueezeExcitation-214           [-1, 1152, 4, 4]               0
          Conv2d-215            [-1, 192, 4, 4]         221,184
     BatchNorm2d-216            [-1, 192, 4, 4]             384
 StochasticDepth-217            [-1, 192, 4, 4]               0
          MBConv-218            [-1, 192, 4, 4]               0
          Conv2d-219           [-1, 1152, 4, 4]         221,184
     BatchNorm2d-220           [-1, 1152, 4, 4]           2,304
            SiLU-221           [-1, 1152, 4, 4]               0
          Conv2d-222           [-1, 1152, 4, 4]          28,800
     BatchNorm2d-223           [-1, 1152, 4, 4]           2,304
            SiLU-224           [-1, 1152, 4, 4]               0
AdaptiveAvgPool2d-225           [-1, 1152, 1, 1]               0
          Conv2d-226             [-1, 48, 1, 1]          55,344
            SiLU-227             [-1, 48, 1, 1]               0
          Conv2d-228           [-1, 1152, 1, 1]          56,448
         Sigmoid-229           [-1, 1152, 1, 1]               0
SqueezeExcitation-230           [-1, 1152, 4, 4]               0
          Conv2d-231            [-1, 192, 4, 4]         221,184
     BatchNorm2d-232            [-1, 192, 4, 4]             384
 StochasticDepth-233            [-1, 192, 4, 4]               0
          MBConv-234            [-1, 192, 4, 4]               0
          Conv2d-235           [-1, 1152, 4, 4]         221,184
     BatchNorm2d-236           [-1, 1152, 4, 4]           2,304
            SiLU-237           [-1, 1152, 4, 4]               0
          Conv2d-238           [-1, 1152, 4, 4]          10,368
     BatchNorm2d-239           [-1, 1152, 4, 4]           2,304
            SiLU-240           [-1, 1152, 4, 4]               0
AdaptiveAvgPool2d-241           [-1, 1152, 1, 1]               0
          Conv2d-242             [-1, 48, 1, 1]          55,344
            SiLU-243             [-1, 48, 1, 1]               0
          Conv2d-244           [-1, 1152, 1, 1]          56,448
         Sigmoid-245           [-1, 1152, 1, 1]               0
SqueezeExcitation-246           [-1, 1152, 4, 4]               0
/home/mvanderbrugge/.conda/envs/photoacoustic-env/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16, 1, 2])) that is different to the input size (torch.Size([16, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/mvanderbrugge/.conda/envs/photoacoustic-env/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([6, 1, 2])) that is different to the input size (torch.Size([6, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
          Conv2d-247            [-1, 320, 4, 4]         368,640
     BatchNorm2d-248            [-1, 320, 4, 4]             640
          MBConv-249            [-1, 320, 4, 4]               0
          Conv2d-250           [-1, 1280, 4, 4]         409,600
     BatchNorm2d-251           [-1, 1280, 4, 4]           2,560
            SiLU-252           [-1, 1280, 4, 4]               0
AdaptiveAvgPool2d-253           [-1, 1280, 1, 1]               0
          Linear-254                  [-1, 128]         163,968
            ReLU-255                  [-1, 128]               0
          Linear-256                    [-1, 2]             258
    EfficientNet-257                    [-1, 2]               0
================================================================
Total params: 4,171,198
Trainable params: 4,171,198
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.06
Forward/backward pass size (MB): 56.85
Params size (MB): 15.91
Estimated Total Size (MB): 72.82
----------------------------------------------------------------
Entering first epoch of training loop
[Epoch 1] Train Loss: 0.0024 | Val Loss: 0.0538
New best model saved to models/EfficientNetIQA/SSIM_GMSD_norm/best_model.pth with val_loss=0.0538
[Epoch 2] Train Loss: 0.0003 | Val Loss: 0.0451
New best model saved to models/EfficientNetIQA/SSIM_GMSD_norm/best_model.pth with val_loss=0.0451
[Epoch 3] Train Loss: 0.0002 | Val Loss: 0.0373
New best model saved to models/EfficientNetIQA/SSIM_GMSD_norm/best_model.pth with val_loss=0.0373
[Epoch 4] Train Loss: 0.0002 | Val Loss: 0.0288
New best model saved to models/EfficientNetIQA/SSIM_GMSD_norm/best_model.pth with val_loss=0.0288
[Epoch 5] Train Loss: 0.0002 | Val Loss: 0.0298
[Epoch 6] Train Loss: 0.0002 | Val Loss: 0.0303
[Epoch 7] Train Loss: 0.0001 | Val Loss: 0.0256
New best model saved to models/EfficientNetIQA/SSIM_GMSD_norm/best_model.pth with val_loss=0.0256
[Epoch 8] Train Loss: 0.0001 | Val Loss: 0.0276
[Epoch 9] Train Loss: 0.0001 | Val Loss: 0.0250
New best model saved to models/EfficientNetIQA/SSIM_GMSD_norm/best_model.pth with val_loss=0.0250
[Epoch 10] Train Loss: 0.0001 | Val Loss: 0.0252
[Epoch 11] Train Loss: 0.0001 | Val Loss: 0.0244
New best model saved to models/EfficientNetIQA/SSIM_GMSD_norm/best_model.pth with val_loss=0.0244
[Epoch 12] Train Loss: 0.0001 | Val Loss: 0.0246
[Epoch 13] Train Loss: 0.0001 | Val Loss: 0.0256
[Epoch 14] Train Loss: 0.0001 | Val Loss: 0.0294
[Epoch 15] Train Loss: 0.0001 | Val Loss: 0.0289
[Epoch 16] Train Loss: 0.0001 | Val Loss: 0.0283
[Epoch 17] Train Loss: 0.0001 | Val Loss: 0.0295
[Epoch 18] Train Loss: 0.0001 | Val Loss: 0.0266
[Epoch 19] Train Loss: 0.0001 | Val Loss: 0.0267
[Epoch 20] Train Loss: 0.0001 | Val Loss: 0.0267
[Epoch 21] Train Loss: 0.0001 | Val Loss: 0.0289
Early stopping triggered after 10 no-improve epochs.
Saved training log to models/EfficientNetIQA/SSIM_GMSD_norm/train_val_loss.csv

JOB STATISTICS
==============
Job ID: 12372271
Cluster: snellius
User/Group: mvanderbrugge/mvanderbrugge
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:03
CPU Efficiency: 0.00% of 12-07:58:30 core-walltime
Job Wall-clock time: 16:26:35
Memory Utilized: 1.68 MB
Memory Efficiency: 0.01% of 32.00 GB (32.00 GB/node)
WARNING: Efficiency statistics can only be obtained after the job has ended as seff tool is based on the accounting database data.
